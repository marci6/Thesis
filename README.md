# Abstract
Scattered across the globe there are billions of IoT devices and smartphones, collecting data through their sensors; all this data can be analysed by means of machine learning to improve a wide variety of sectors. Nowadays, modelsâ€™ training and inference are performed on Cloud Computing, but there exists another possibility that offers many advantages, such as faster inference and higher security: Edge Computing. The successful application of ML on the edge requires some adaptation since edge devices have constrained hardware capabilities, thus our quest to find the effects that model compression, an essential characteristic of edge computing, have on learning, in particular on continual learning. We compared the standard multilayer perceptron with its stochastic version on multiple well-known datasets to find out whether stochastic models are better suited for Quantization Aware Training. We concluded that Bayesian Neural Networks offer an advantage when quantized, especially in the context of continual learning.
